{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b2ad6c-eb4c-427b-8080-bc9276efeff1",
   "metadata": {},
   "source": [
    "# Spark with Iceberg on S3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b46933-210f-486a-aae7-84d7f7f1654d",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e437c-f9f1-4f82-8dc9-c9ac1346e53c",
   "metadata": {},
   "source": [
    "#### 1. Make sure maven is configured with java 11\n",
    "1. Hadoop 3.x: Requires Java 8 or Java 11, so point to java 11 version in `PATH`\n",
    "2. Check `mvn --version`\n",
    "3. If not configured, setup maven https://phoenixnap.com/kb/install-maven-windows\n",
    "\n",
    "#### 2. Add `winutils.exe` to hadoop home\n",
    "1. Create a directory for hadoop home (ex: `C:\\hadoop`)\n",
    "2. Download `winutils.exe`, a popular repository having this is https://github.com/steveloughran/winutils,   \n",
    "   File link: https://github.com/steveloughran/winutils/blob/master/hadoop-3.0.0/bin/winutils.exe\n",
    "4. Create a folder in hadoop home named `bin` and add `winutils.exe` to it\n",
    "5. Add environment variable, `HADOOP_HOME` (`C:\\hadoop`)\n",
    "6. Add  `HADOOP_HOME\\bin` to `PATH` (`%HADOOP_HOME%\\bin`)\n",
    "\n",
    "#### 3. Add below AWS related values to environment variables\n",
    "\n",
    "These can be taken from https://sysco-sso.awsapps.com/start/#\n",
    "\n",
    "1. `AWS_REGION`\n",
    "2. `AWS_ACCESS_KEY_ID`\n",
    "3. `AWS_SECRET_ACCESS_KEY`\n",
    "4. `AWS_SESSION_TOKEN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a2754-fefe-4caf-9811-2e9558fe6aef",
   "metadata": {},
   "source": [
    "## Note\n",
    "> A database named `test_uph_iceberg` has been created in AWS Glue. It will be used in this example.   \n",
    "Link: https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#/v2/data-catalog/databases/view/test_uph_iceberg?catalogId=909082630066\n",
    "\n",
    "> Catelog name is taken as `uph_rnd`. This can be any name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13542d-b152-419c-b6f4-8ae02b0f7f73",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f4ce2e-f858-45bf-897e-407b5eea5c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ICEBERG_S3_WAREHOUSE = \"s3://cx-unique-purchase-data-non-prod/dev/test_data\"\n",
    "AWS_REGION = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0a9ff0-42fc-4afc-bbe0-5d3d0825e32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created in 0.7298665046691895 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName('Spark Iceberg Example')\n",
    "    \n",
    "    .config(\"spark.network.timeout\", '10000s')\n",
    "    .config('spark.sql.autoBroadcastJoinThreshold', -1)\n",
    "    .config('spark.shuffle.consolidateFiles', True)\n",
    "    .config('spark.dynamicAllocation.enabled', False)\n",
    "    .config(\"spark.serializer\", 'org.apache.spark.serializer.KryoSerializer')\n",
    "    .config('spark.shuffle.service.enabled', False)\n",
    "    .config('spark.hadoop.fs.s3.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", f\"s3.{AWS_REGION}.amazonaws.com\")\n",
    "\n",
    "    .config('spark.jars.packages', \n",
    "            'org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.1,'\n",
    "            'org.apache.iceberg:iceberg-aws:1.4.1,'\n",
    "            'org.apache.iceberg:iceberg-aws-bundle:1.4.1,')\n",
    "\n",
    "    .config('spark.partial-progress.enabled', True)\n",
    "    \n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    \n",
    "    .config(\"spark.sql.catalog.uph_rnd\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.uph_rnd.glue-database\", \"test_uph_iceberg\")\n",
    "    .config(\"spark.sql.catalog.uph_rnd.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "    .config(\"spark.sql.catalog.uph_rnd.warehouse\", ICEBERG_S3_WAREHOUSE)\n",
    "    .config(\"spark.sql.catalog.uph_rnd.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Spark session created in {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4990f-4911-46da-b467-c675a6155424",
   "metadata": {},
   "source": [
    "## Create Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8de9c9d-9e42-49eb-b753-f6cf408e1e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an Iceberg table using DDL (if not already created)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS uph_rnd.test_uph_iceberg.users (\n",
    "        UserID INT,\n",
    "        Username STRING,\n",
    "        Email STRING,\n",
    "        SignupDate DATE,\n",
    "        LastLogin DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (LastLogin)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea51cb52-d1b9-4c85-8046-e381a279b1cf",
   "metadata": {},
   "source": [
    "## Check Tables in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "883a0713-581c-4270-931f-6ad2104ebbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------+\n",
      "|       namespace|tableName|isTemporary|\n",
      "+----------------+---------+-----------+\n",
      "|test_uph_iceberg|    users|      false|\n",
      "+----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check tables in test_uph_iceberg database\n",
    "spark.sql(\"SHOW TABLES IN uph_rnd.test_uph_iceberg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ce28f-98a5-42de-a9cb-3da4ac8bbeb9",
   "metadata": {},
   "source": [
    "## Load Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e78cbf7-fde1-4494-90ee-a68977a0b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------------------+----------+----------+\n",
      "|UserID|    Username|               Email|SignupDate| LastLogin|\n",
      "+------+------------+--------------------+----------+----------+\n",
      "|     1|     johndoe| johndoe@example.com|2024-01-01|2024-10-01|\n",
      "|     2|     janedoe| janedoe@example.com|2024-02-15|2024-10-12|\n",
      "|     3|    bobsmith|bobsmith@example.com|2024-03-20|2024-09-28|\n",
      "|     4|  alicejones|alicejones@exampl...|2024-04-10|2024-10-05|\n",
      "|     5|charliebrown|charliebrown@exam...|2024-05-05|2024-10-10|\n",
      "+------+------------+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV example data from a CSV file\n",
    "csv_file_path = \"example_user_list.csv\"\n",
    "df_users = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df_users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19141ec-03c5-48f5-8e65-4db050e62c0e",
   "metadata": {},
   "source": [
    "## Insert Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06f1ef8f-f9d3-4b0a-9a84-42ba8826c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data into temporary view\n",
    "df_users.createOrReplaceTempView(\"temp_users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b908f25-3a51-49eb-a4da-3a52d39e0a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert data from the temporary view into the Iceberg table\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO uph_rnd.test_uph_iceberg.users\n",
    "    SELECT * FROM temp_users\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09ccd4-ac7e-4a04-9f62-667094364e3a",
   "metadata": {},
   "source": [
    "## Read Data in Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f3c69eb-1eb3-45f4-a015-109e7ad70837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------------------+----------+----------+\n",
      "|UserID|    Username|               Email|SignupDate| LastLogin|\n",
      "+------+------------+--------------------+----------+----------+\n",
      "|     3|    bobsmith|bobsmith@example.com|2024-03-20|2024-09-28|\n",
      "|     1|     johndoe| johndoe@example.com|2024-01-01|2024-10-01|\n",
      "|     4|  alicejones|alicejones@exampl...|2024-04-10|2024-10-05|\n",
      "|     5|charliebrown|charliebrown@exam...|2024-05-05|2024-10-10|\n",
      "|     2|     janedoe| janedoe@example.com|2024-02-15|2024-10-12|\n",
      "+------+------------+--------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM uph_rnd.test_uph_iceberg.users\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a7db7-1f85-4844-adf7-ddc9957f1fc8",
   "metadata": {},
   "source": [
    "## Truncate Data in Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83ca381e-119f-4d7e-afb2-1fd2c6eecba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate table\n",
    "spark.sql(\"TRUNCATE TABLE uph_rnd.test_uph_iceberg.users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88311aef-0da3-4f2f-8fb5-dcd7c6105d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+-------------+---------+--------------------+--------------------+-------+\n",
      "|customer_id|cust_id_int|seller_id|sub_seller_id|list_type|             list_id|          updated_at|opco_id|\n",
      "+-----------+-----------+---------+-------------+---------+--------------------+--------------------+-------+\n",
      "|     000000|          0|     USBL|         USBL|    Block|056_common_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "|     000000|          0|     USBL|         USBL|  Can Buy|          056_canbuy|2024-03-12 12:47:...|     56|\n",
      "|     000006|          6|     USBL|         USBL|    Block|056_common_cannotbuy|2024-09-24 23:35:...|     56|\n",
      "|     000008|          8|     USBL|         USBL|  Can Buy|          056_canbuy|2024-09-24 23:35:...|     56|\n",
      "|     000009|          9|     USBL|         USBL|    Block|056_common_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "|     000009|          9|     USBL|         USBL|  Can Buy|          056_canbuy|2024-03-12 12:47:...|     56|\n",
      "|     000016|         16|     USBL|         USBL|  Can Buy|          056_canbuy|2024-03-12 12:47:...|     56|\n",
      "|     000017|         17|     USBL|         USBL|    Block|056_000017_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "|     000017|         17|     USBL|         USBL|    Block|056_common_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "|     000018|         18|     USBL|         USBL|  Can Buy|          056_canbuy|2024-03-12 12:47:...|     56|\n",
      "|     000019|         19|     USBL|         USBL|    Block|056_common_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "|     000019|         19|     USBL|         USBL|  Can Buy|          056_canbuy|2024-03-12 12:47:...|     56|\n",
      "|     000020|         20|     USBL|         USBL|    Block|056_000020_cannotbuy|2024-09-24 23:35:...|     56|\n",
      "|     000025|         25|     USBL|         USBL|    Block|056_common_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "|     000026|         26|     USBL|         USBL|    Block|056_000026_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "|     000026|         26|     USBL|         USBL|    Block|056_common_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "|     000029|         29|     USBL|         USBL|    Block|056_common_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "|     000029|         29|     USBL|         USBL|  Can Buy|          056_canbuy|2024-03-12 12:47:...|     56|\n",
      "|     000032|         32|     USBL|         USBL|  Can Buy|          056_canbuy|2024-09-24 23:35:...|     56|\n",
      "|     000040|         40|     USBL|         USBL|    Block|056_000040_cannotbuy|2024-03-12 12:47:...|     56|\n",
      "+-----------+-----------+---------+-------------+---------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_parquet_path = \"s3://cx-staging-glue-catalog/dev/sysco_src_restrictions_product_attributes_staging.parquet\"\n",
    "parquet_df = spark.read.parquet(s3_parquet_path)\n",
    "parquet_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uph_rnd_env)",
   "language": "python",
   "name": "uph_rnd_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
